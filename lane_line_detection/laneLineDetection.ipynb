{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"laneLineDetection.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"15XqSrierU0RYlbj1jLhz5fN-AVIylqYs","authorship_tag":"ABX9TyNWOugd5lFftRwl8hp4sAdc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"rZJSWagnN5FO","colab_type":"code","colab":{}},"source":["import numpy as np\n","import cv2\n","\n","\n","class Line:\n","    \"\"\"\n","    A Line is defined from two points (x1, y1) and (x2, y2) as follows:\n","    y - y1 = (y2 - y1) / (x2 - x1) * (x - x1)\n","    Each line has its own slope and intercept (bias).\n","    \"\"\"\n","    def __init__(self, x1, y1, x2, y2):\n","\n","        self.x1 = np.float32(x1)\n","        self.y1 = np.float32(y1)\n","        self.x2 = np.float32(x2)\n","        self.y2 = np.float32(y2)\n","\n","        self.slope = self.compute_slope()\n","        self.bias = self.compute_bias()\n","\n","    def compute_slope(self):\n","        return (self.y2 - self.y1) / (self.x2 - self.x1 + np.finfo(float).eps)\n","\n","    def compute_bias(self):\n","        return self.y1 - self.slope * self.x1\n","\n","    def get_coords(self):\n","        return np.array([self.x1, self.y1, self.x2, self.y2])\n","\n","    def set_coords(self, x1, y1, x2, y2):\n","        self.x1 = x1\n","        self.y1 = y1\n","        self.x2 = x2\n","        self.y2 = y2\n","\n","    def draw(self, img, color=[255, 0, 0], thickness=10):\n","        cv2.line(img, (self.x1, self.y1), (self.x2, self.y2), color, thickness)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"71TOcmIsN3Yb","colab_type":"code","colab":{}},"source":["import numpy as np\n","import cv2\n","# from Line import Line\n","\n","\n","def region_of_interest(img, vertices):\n","    \"\"\"\n","    Applies an image mask.\n","\n","    Only keeps the region of the image defined by the polygon\n","    formed from `vertices`. The rest of the image is set to black.\n","    \"\"\"\n","\n","    # defining a blank mask to start with\n","    mask = np.zeros_like(img)\n","\n","    # defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n","    if len(img.shape) > 2:\n","        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n","        ignore_mask_color = (255,) * channel_count\n","    else:\n","        ignore_mask_color = 255\n","\n","    # filling pixels inside the polygon defined by \"vertices\" with the fill color\n","    cv2.fillPoly(mask, vertices, ignore_mask_color)\n","\n","    # returning the image only where mask pixels are nonzero\n","    masked_image = cv2.bitwise_and(img, mask)\n","\n","    return masked_image, mask\n","\n","\n","def hough_lines_detection(img, rho, theta, threshold, min_line_len, max_line_gap):\n","    \"\"\"\n","    `img` should be the output of a Canny transform.\n","    \"\"\"\n","    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len,\n","                            maxLineGap=max_line_gap)\n","    return lines\n","\n","\n","def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n","    \"\"\"\n","    Returns resulting blend image computed as follows:\n","\n","    initial_img * α + img * β + λ\n","    \"\"\"\n","    img = np.uint8(img)\n","    if len(img.shape) is 2:\n","        img = np.dstack((img, np.zeros_like(img), np.zeros_like(img)))\n","\n","    return cv2.addWeighted(initial_img, α, img, β, λ)\n","\n","\n","def compute_lane_from_candidates(line_candidates, img_shape):\n","    \"\"\"\n","    Compute lines that approximate the position of both road lanes.\n","\n","    :param line_candidates: lines from hough transform\n","    :param img_shape: shape of image to which hough transform was applied\n","    :return: lines that approximate left and right lane position\n","    \"\"\"\n","\n","    # separate candidate lines according to their slope\n","    pos_lines = [l for l in line_candidates if l.slope > 0]\n","    neg_lines = [l for l in line_candidates if l.slope < 0]\n","\n","    # interpolate biases and slopes to compute equation of line that approximates left lane\n","    # median is employed to filter outliers\n","    neg_bias = np.median([l.bias for l in neg_lines]).astype(int)\n","    neg_slope = np.median([l.slope for l in neg_lines])\n","    x1, y1 = 0, neg_bias\n","    x2, y2 = -np.int32(np.round(neg_bias / neg_slope)), 0\n","    left_lane = Line(x1, y1, x2, y2)\n","\n","    # interpolate biases and slopes to compute equation of line that approximates right lane\n","    # median is employed to filter outliers\n","    lane_right_bias = np.median([l.bias for l in pos_lines]).astype(int)\n","    lane_right_slope = np.median([l.slope for l in pos_lines])\n","    x1, y1 = 0, lane_right_bias\n","    x2, y2 = np.int32(np.round((img_shape[0] - lane_right_bias) / lane_right_slope)), img_shape[0]\n","    right_lane = Line(x1, y1, x2, y2)\n","\n","    return left_lane, right_lane\n","\n","\n","def get_lane_lines(color_image, solid_lines=True):\n","    \"\"\"\n","    This function take as input a color road frame and tries to infer the lane lines in the image.\n","    :param color_image: input frame\n","    :param solid_lines: if True, only selected lane lines are returned. If False, all candidate lines are returned.\n","    :return: list of (candidate) lane lines.\n","    \"\"\"\n","    # resize to 960 x 540\n","    color_image = cv2.resize(color_image, (960, 540))\n","\n","    # convert to grayscale\n","    img_gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n","\n","    # perform gaussian blur\n","    img_blur = cv2.GaussianBlur(img_gray, (17, 17), 0)\n","\n","    # perform edge detection\n","    img_edge = cv2.Canny(img_blur, threshold1=50, threshold2=80)\n","\n","    # perform hough transform\n","    detected_lines = hough_lines_detection(img=img_edge,\n","                                           rho=2,\n","                                           theta=np.pi / 180,\n","                                           threshold=1,\n","                                           min_line_len=15,\n","                                           max_line_gap=5)\n","\n","    # convert (x1, y1, x2, y2) tuples into Lines\n","    detected_lines = [Line(l[0][0], l[0][1], l[0][2], l[0][3]) for l in detected_lines]\n","\n","    # if 'solid_lines' infer the two lane lines\n","    if solid_lines:\n","        candidate_lines = []\n","        for line in detected_lines:\n","                # consider only lines with slope between 30 and 60 degrees\n","                if 0.5 <= np.abs(line.slope) <= 2:\n","                    candidate_lines.append(line)\n","        # interpolate lines candidates to find both lanes\n","        lane_lines = compute_lane_from_candidates(candidate_lines, img_gray.shape)\n","    else:\n","        # if not solid_lines, just return the hough transform output\n","        lane_lines = detected_lines\n","\n","    return lane_lines\n","\n","\n","def smoothen_over_time(lane_lines):\n","    \"\"\"\n","    Smooth the lane line inference over a window of frames and returns the average lines.\n","    \"\"\"\n","\n","    avg_line_lt = np.zeros((len(lane_lines), 4))\n","    avg_line_rt = np.zeros((len(lane_lines), 4))\n","\n","    for t in range(0, len(lane_lines)):\n","        avg_line_lt[t] += lane_lines[t][0].get_coords()\n","        avg_line_rt[t] += lane_lines[t][1].get_coords()\n","\n","    return Line(*np.mean(avg_line_lt, axis=0)), Line(*np.mean(avg_line_rt, axis=0))\n","\n","\n","def color_frame_pipeline(frames, solid_lines=True, temporal_smoothing=True):\n","    \"\"\"\n","    Entry point for lane detection pipeline. Takes as input a list of frames (RGB) and returns an image (RGB)\n","    with overlaid the inferred road lanes. Eventually, len(frames)==1 in the case of a single image.\n","    \"\"\"\n","    is_videoclip = len(frames) > 0\n","\n","    img_h, img_w = frames[0].shape[0], frames[0].shape[1]\n","\n","    lane_lines = []\n","    for t in range(0, len(frames)):\n","        inferred_lanes = get_lane_lines(color_image=frames[t], solid_lines=solid_lines)\n","        lane_lines.append(inferred_lanes)\n","\n","    if temporal_smoothing and solid_lines:\n","        lane_lines = smoothen_over_time(lane_lines)\n","    else:\n","        lane_lines = lane_lines[0]\n","\n","    # prepare empty mask on which lines are drawn\n","    line_img = np.zeros(shape=(img_h, img_w))\n","\n","    # draw lanes found\n","    for lane in lane_lines:\n","        lane.draw(line_img)\n","\n","    # keep only region of interest by masking\n","    vertices = np.array([[(50, img_h),\n","                          (450, 310),\n","                          (490, 310),\n","                          (img_w - 50, img_h)]],\n","                        dtype=np.int32)\n","    img_masked, _ = region_of_interest(line_img, vertices)\n","\n","    # make blend on color image\n","    img_color = frames[-1] if is_videoclip else frames[0]\n","    img_blend = weighted_img(img_masked, img_color, α=0.8, β=1., λ=0.)\n","\n","    return img_blend\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kj_xZ1HYN-eA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LoRwYBXG8KYT8q1jI-ywiHwg7LKAEpop"},"outputId":"46807a4a-e565-4744-a353-42056aff4f37","executionInfo":{"status":"ok","timestamp":1589314950916,"user_tz":240,"elapsed":53352,"user":{"displayName":"Arun Kumar A","photoUrl":"","userId":"12591767503691125936"}}},"source":["import matplotlib.pyplot as plt\n","import matplotlib as mp\n","import cv2\n","import os\n","from os.path import join, basename\n","from collections import deque\n","# from lane_detection import color_frame_pipeline\n","\n","\n","if __name__ == '__main__':\n","\n","    resize_h, resize_w = 540, 960\n","\n","    verbose = True\n","    if verbose:\n","        plot_backend = mp.get_backend()\n","        plt.ion()\n","        figManager = plt.get_current_fig_manager()\n","        if plot_backend == 'Qt4Agg':\n","          figManager.window.showMaximized()\n","\n","    # test on images\n","    test_images_dir = join('/content/drive/My Drive/Colab Notebooks/self_driving_example/land_detection/data', 'test_images')\n","    test_images = [join(test_images_dir, name) for name in os.listdir(test_images_dir)]\n","\n","    # for test_img in test_images:\n","\n","    #     print('Processing image: {}'.format(test_img))\n","\n","    #     out_path = join('/content/drive/My Drive/Colab Notebooks/self_driving_example/land_detection/out', 'images', basename(test_img))\n","    #     in_image = cv2.cvtColor(cv2.imread(test_img, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n","    #     out_image = color_frame_pipeline([in_image], solid_lines=True)\n","    #     cv2.imwrite(out_path, cv2.cvtColor(out_image, cv2.COLOR_RGB2BGR))\n","    #     if verbose:\n","    #         plt.imshow(out_image)\n","    #         plt.waitforbuttonpress()\n","    # plt.close('all')\n","\n","    # test on videos\n","    test_videos_dir = join('/content/drive/My Drive/Colab Notebooks/self_driving_example/land_detection/data', 'test_videos')\n","    test_videos = [join(test_videos_dir, name) for name in os.listdir(test_videos_dir)]\n","\n","    for test_video in test_videos:\n","\n","        print('Processing video: {}'.format(test_video))\n","\n","        cap = cv2.VideoCapture(test_video)\n","        out = cv2.VideoWriter(join('/content/drive/My Drive/Colab Notebooks/self_driving_example/land_detection/out', 'videos', basename(test_video)),\n","                              fourcc=cv2.VideoWriter_fourcc(*'DIVX'),\n","                              fps=20.0, frameSize=(resize_w, resize_h))\n","\n","        frame_buffer = deque(maxlen=10)\n","        while cap.isOpened():\n","            ret, color_frame = cap.read()\n","            if ret:\n","                color_frame = cv2.cvtColor(color_frame, cv2.COLOR_BGR2RGB)\n","                color_frame = cv2.resize(color_frame, (resize_w, resize_h))\n","                frame_buffer.append(color_frame)\n","                blend_frame = color_frame_pipeline(frames=frame_buffer, solid_lines=True, temporal_smoothing=True)\n","                out.write(cv2.cvtColor(blend_frame, cv2.COLOR_RGB2BGR))\n","                # cv2.imshow not working in colab\n","                from google.colab.patches import cv2_imshow\n","                cv2_imshow(blend_frame), cv2.waitKey(1)\n","                # cv2.imshow('blend', cv2.cvtColor(blend_frame, cv2.COLOR_RGB2BGR)), cv2.waitKey(1)\n","            else:\n","                break\n","        cap.release()\n","        out.release()\n","        cv2.destroyAllWindows()\n","\n","\n","\n"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"E2Xf4SCJORU0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f101136e-9c0b-4150-f40a-76dce42a8449","executionInfo":{"status":"ok","timestamp":1589311397305,"user_tz":240,"elapsed":1079,"user":{"displayName":"Arun Kumar A","photoUrl":"","userId":"12591767503691125936"}}},"source":["# plot_backend = mp.get_backend()\n","# mng = plt.get_current_fig_manager()\n","# if plot_backend == 'TkAgg':\n","#   mng.resize(*mng.window.maxsize())\n","# elif plot_backend == 'wxAgg':\n","#   mng.frame.Maximize(True)\n","# elif plot_backend == 'Qt4Agg':\n","#   mng.window.showMaximized()\n"],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"xHFp_t9aVe2i","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}